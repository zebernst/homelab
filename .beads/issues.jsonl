{"id":"homelab-04z","title":"Priority Classes for Workload Scheduling","description":"Establish a tiered PriorityClass hierarchy so critical infrastructure survives resource pressure.\n\n## Current State\n\n**Existing files (uncommitted, need updating):**\n- `priority-class/control-plane-critical.yaml` (1500) ✓\n- `priority-class/network-critical.yaml` (1750) ✓\n- `priority-class/security-critical.yaml` (1400) - to be removed\n- `priority-class/external-facing.yaml` (200) - needs value change to 500\n- `priority-class/media-high.yaml` (300) - rename to media-core\n\n**Need to create:**\n- `storage-critical.yaml` (2000)\n- `database-critical.yaml` (1200)\n- `internal-only.yaml` (100)\n- `background.yaml` (50)\n- `best-effort.yaml` (0)\n\n**Already using built-in classes:**\n- rook-ceph: `system-cluster-critical`\n- generic-device-plugin: `system-node-critical`\n\n## Final Hierarchy\n\n| Class | Value | Workloads |\n|-------|-------|-----------|\n| storage-critical | 2000 | rook-ceph*, openebs, synology-csi, volsync, snapshot-controller |\n| network-critical | 1750 | cloudflared, external-dns, ingress-nginx, tailscale-operator |\n| control-plane-critical | 1500 | flux, cert-manager, external-secrets, prometheus-stack, grafana, loki, metrics-server |\n| database-critical | 1200 | cloudnative-pg, dragonfly, mariadb |\n| external-facing | 500 | plex, wizarr, overseerr, rxresume, gatus, kromgo |\n| media-core | 300 | sonarr, radarr, lidarr, prowlarr, bazarr, qbittorrent, autobrr, tautulli, calibre-web, maintainerr, booklore |\n| internal-only | 100 | glance, atuin, radicale, nocodb, mealie, paperless, homebox, hajimari, it-tools |\n| background | 50 | recyclarr, unpackerr, cross-seed, omegabrr, flaresolverr, stash |\n| best-effort | 0 | ollama, local-ai, steam, openbooks, minecraft |\n\n*rook-ceph already uses system-cluster-critical\n\n## Implementation Steps\n\n### Phase 1: Create/Update PriorityClass files\n1. [ ] Update `external-facing.yaml`: change value 200 → 500\n2. [ ] Rename `media-high.yaml` → `media-core.yaml`\n3. [ ] Delete `security-critical.yaml` (merged into control-plane)\n4. [ ] Create `storage-critical.yaml` (2000)\n5. [ ] Create `database-critical.yaml` (1200)\n6. [ ] Create `internal-only.yaml` (100)\n7. [ ] Create `background.yaml` (50)\n8. [ ] Create `best-effort.yaml` (0)\n9. [ ] Update `kube-system/kustomization.yaml` with new files\n\n### Phase 2: Apply to workloads (by namespace)\n10. [ ] kube-system: Apply to cilium, coredns, metrics-server, etc.\n11. [ ] rook-ceph: Verify system-cluster-critical (already set)\n12. [ ] openebs-system: Apply storage-critical\n13. [ ] volsync-system: Apply storage-critical\n14. [ ] network: Apply network-critical to cloudflared, external-dns, etc.\n15. [ ] flux-system: Apply control-plane-critical\n16. [ ] cert-manager: Apply control-plane-critical\n17. [ ] external-secrets: Apply control-plane-critical\n18. [ ] observability: Apply control-plane-critical\n19. [ ] database: Apply database-critical\n20. [ ] media: Apply external-facing (plex, wizarr, overseerr) and media-core (rest)\n21. [ ] downloads: Apply media-core and background\n22. [ ] self-hosted: Apply internal-only\n23. [ ] ai: Apply best-effort\n24. [ ] games: Apply best-effort\n\n### Phase 3: Validation\n25. [ ] Deploy changes and verify PriorityClasses exist\n26. [ ] Verify pods have correct priorityClassName\n27. [ ] Test preemption under resource pressure (optional)\n\n## Files Changed\n- `kubernetes/apps/kube-system/priority-class/*.yaml`\n- `kubernetes/apps/kube-system/kustomization.yaml`\n- HelmReleases across all namespaces (priorityClassName field)","status":"in_progress","priority":2,"issue_type":"feature","owner":"zach.bernstein@smarterdx.com","created_at":"2026-01-17T13:26:08.262193-06:00","created_by":"Zach Bernstein","updated_at":"2026-02-17T14:45:46.808706-06:00","comments":[{"id":1,"issue_id":"homelab-04z","author":"Zach Bernstein","text":"## Refined Priority Class Hierarchy\n\nBased on codebase exploration of 77 apps across 17 namespaces, here is the proposed hierarchy:\n\n| Priority Class | Value | Description |\n|----------------|-------|-------------|\n| **system-cluster-critical** | (built-in) | K8s system (cilium, coredns) |\n| **system-node-critical** | (built-in) | Node-level (device plugins) |\n| **storage-critical** | 2000 | Storage infrastructure |\n| **network-critical** | 1750 | External connectivity |\n| **control-plane-critical** | 1500 | GitOps \u0026 observability |\n| **database-critical** | 1200 | Stateful data services |\n| **external-facing** | 500 | Public internet access |\n| **media-core** | 300 | Media pipeline |\n| **internal-only** | 100 | Internal tools |\n| **background** | 50 | Batch processing |\n| **best-effort** | 0 | Preemptable workloads |\n\n## Workload Assignments\n\n**storage-critical (2000):** rook-ceph, openebs, synology-csi, volsync, snapshot-controller\n\n**network-critical (1750):** cloudflared, external-dns, ingress-nginx, tailscale-operator\n\n**control-plane-critical (1500):** flux, cert-manager, external-secrets, prometheus-stack, grafana, loki, metrics-server\n\n**database-critical (1200):** cloudnative-pg, dragonfly, mariadb\n\n**external-facing (500):** plex, wizarr, overseerr, rxresume, gatus, kromgo\n\n**media-core (300):** sonarr, radarr, lidarr, prowlarr, bazarr, qbittorrent, autobrr, tautulli, calibre-web, maintainerr, booklore\n\n**internal-only (100):** glance, atuin, radicale, nocodb, mealie, paperless, homebox, hajimari, it-tools\n\n**background (50):** recyclarr, unpackerr, cross-seed, omegabrr, flaresolverr, stash\n\n**best-effort (0):** ollama, local-ai, steam, openbooks, minecraft\n\n## Notes\n- Apps already using built-in classes: rook-ceph (system-cluster-critical), generic-device-plugin (system-node-critical)\n- Tailscale-specific class removed; apps will be accessible via both Tailscale and internal LAN via Gateway API migration","created_at":"2026-01-17T19:33:41Z"}]}
{"id":"homelab-0q3","title":"Deploy standalone Envoy Gateway","description":"Deploy a standalone Envoy Gateway for more customization than Cilium's built-in Gateway API implementation.\n\n## Rationale\n\nCilium's built-in Gateway API is minimal. A standalone Envoy Gateway provides:\n- More configuration options (compression, retries, keepalives)\n- Better observability (Prometheus metrics)\n- Advanced traffic policies (ClientTrafficPolicy, BackendTrafficPolicy)\n- HTTP/2 and HTTP/3 support\n- Fine-grained TLS configuration\n\n## Reference Implementation\n\nBased on: https://github.com/buroa/k8s-gitops/tree/main/kubernetes/apps/networking/envoy-gateway\n\n## Architecture\n\n```\n                   Cilium LB IPAM\n                        │\n            ┌───────────┴───────────┐\n            ▼                       ▼\n    192.168.20.XX            192.168.20.YY\n    envoy-external           envoy-internal\n            │                       │\n            ▼                       ▼\n      HTTPRoutes               HTTPRoutes\n    (public apps)           (internal apps)\n```\n\n## Key Features to Configure\n\n### EnvoyProxy Resource\n- 2 replicas for HA\n- Resource limits (100m CPU, 2Gi memory)\n- 180s drain timeout for graceful shutdown\n- Prometheus metrics with Zstd compression\n- Info-level logging\n\n### GatewayClass\n- Named `envoy` with controller `gateway.envoyproxy.io/gatewayclass-controller`\n- References EnvoyProxy config via parametersRef\n\n### Gateways (2)\n- **envoy-external**: Public traffic via Cloudflare Tunnel\n- **envoy-internal**: LAN + Tailscale traffic\n- Both use `lbipam.cilium.io/ips` for static IP assignment\n- HTTP (port 80) + HTTPS (port 443) listeners\n\n### BackendTrafficPolicy\n- Compression: Zstd, Brotli, Gzip\n- Retries: 2 retries on connection reset\n- TCP keepalive enabled\n\n### ClientTrafficPolicy\n- Client IP detection via X-Forwarded-For (from pod CIDR)\n- TLS 1.2 minimum\n- HTTP/2 and HTTP/3 support\n\n### HTTPRoute (redirect)\n- HTTP → HTTPS redirect (301)\n\n## Implementation Steps\n\n### Phase 1: Deploy Envoy Gateway Operator\n1. [ ] Create `kubernetes/apps/network/envoy-gateway/` directory structure\n2. [ ] Create `ks.yaml` with two Kustomizations (app + proxy)\n3. [ ] Create `app/helmrelease.yaml` using `oci://docker.io/envoyproxy/gateway-helm`\n4. [ ] Configure provider for Kubernetes + GatewayNamespace deployment\n\n### Phase 2: Configure Proxy \u0026 Gateways\n5. [ ] Create `proxy/envoy.yaml` with EnvoyProxy, GatewayClass, Gateways\n6. [ ] Allocate IPs from Cilium LB IPAM pool\n7. [ ] Create BackendTrafficPolicy for compression/retries\n8. [ ] Create ClientTrafficPolicy for TLS/HTTP2/HTTP3\n9. [ ] Create HTTP→HTTPS redirect HTTPRoute\n10. [ ] Create `proxy/certificate.yaml` for TLS (reference existing cert or create new)\n\n### Phase 3: Observability\n11. [ ] Create `proxy/podmonitor.yaml` for Prometheus scraping\n12. [ ] Verify metrics in Grafana\n\n### Phase 4: Migration\n13. [ ] Test with a single app (e.g., echo-server)\n14. [ ] Migrate apps from Cilium Gateway to Envoy Gateway\n15. [ ] Update existing HTTPRoutes to reference new gateways\n16. [ ] Remove old Cilium gateways when migration complete\n\n## Files to Create\n```\nkubernetes/apps/network/envoy-gateway/\n├── ks.yaml\n├── app/\n│   ├── kustomization.yaml\n│   └── helmrelease.yaml\n└── proxy/\n    ├── kustomization.yaml\n    ├── envoy.yaml          # EnvoyProxy, GatewayClass, Gateways, Policies\n    ├── certificate.yaml\n    └── podmonitor.yaml\n```\n\n## Helm Chart\n```yaml\nrepository: oci://docker.io/envoyproxy/gateway-helm\nchart: gateway-helm\nversion: v1.3.x  # or latest stable\n```\n\n## Cilium Integration\n- Uses `lbipam.cilium.io/ips` annotation for static LoadBalancer IPs\n- Cilium continues to handle L3/L4 load balancing\n- Envoy handles L7 (HTTP/HTTPS) processing\n\n## Related Issues\n- May interact with homelab-6ae (Gateway API migration) - could replace Cilium gateways entirely\n- May interact with homelab-6rb (TinyCA) - certificates for internal gateway\n\n## Resources\n- [Envoy Gateway Docs](https://gateway.envoyproxy.io/)\n- [Reference: buroa/k8s-gitops](https://github.com/buroa/k8s-gitops/tree/main/kubernetes/apps/networking/envoy-gateway)\n- [Gateway API Spec](https://gateway-api.sigs.k8s.io/)","status":"in_progress","priority":2,"issue_type":"feature","owner":"zach.bernstein@smarterdx.com","created_at":"2026-01-17T13:51:59.818053-06:00","created_by":"Zach Bernstein","updated_at":"2026-02-17T14:45:47.039205-06:00","comments":[{"id":2,"issue_id":"homelab-0q3","author":"Zach Bernstein","text":"**Decision needed:** Should this issue be completed BEFORE homelab-6ae (Gateway API migration)?\n\nIf yes, apps would migrate directly to Envoy Gateway instead of Cilium Gateway.\n\nCurrent dependency options:\n1. **Envoy first**: Deploy Envoy Gateway, then migrate apps to it (homelab-6ae depends on this)\n2. **Parallel**: Deploy Envoy Gateway alongside Cilium, migrate later\n3. **Replace in-place**: Complete homelab-6ae with Cilium first, then swap to Envoy\n\nRecommend option 1 for cleaner migration path.","created_at":"2026-01-17T19:52:30Z"}]}
{"id":"homelab-21f","title":"Deploy Qui as qBittorrent alternative UI","description":"Deploy Qui as a modern alternative web UI for qBittorrent.\n\n## Current Setup (Your Repo)\n\n```yaml\n# kubernetes/apps/downloads/qbittorrent/app/helmrelease.yaml\nimage:\n  repository: ghcr.io/home-operations/qbittorrent\n  tag: 5.1.4\n# Using default qBittorrent WebUI\n```\n\nCurrently using the built-in qBittorrent WebUI.\n\n## Reference Setup (buroa \u0026 onedr0p)\n\nBoth repos deploy **Qui** alongside qBittorrent:\n\n- Modern, responsive UI for qBittorrent\n- Better mobile experience\n- Additional features and customization\n- Connects to qBittorrent API\n\n## Why Deploy?\n\n1. **Better UX**: Modern, responsive design\n2. **Mobile-Friendly**: Works well on phones/tablets\n3. **Features**: Additional management capabilities\n4. **Customization**: Theming and layout options\n\n## Implementation Steps\n\n1. [ ] Research Qui (find source repo/container image)\n2. [ ] Create Qui deployment alongside qBittorrent\n3. [ ] Configure to connect to qBittorrent API\n4. [ ] Expose via ingress (internal or tailscale)\n5. [ ] Test functionality\n\n## Files to Create\n\n```\nkubernetes/apps/downloads/qui/\n├── ks.yaml\n└── app/\n    ├── kustomization.yaml\n    └── helmrelease.yaml\n```\n\n## Configuration\n\nQui needs to connect to qBittorrent's API:\n```yaml\nenv:\n  QBITTORRENT_HOST: qbittorrent.downloads.svc.cluster.local\n  QBITTORRENT_PORT: 80\n```\n\n## Resources\n\n- Search for Qui qBittorrent UI\n- Check reference repos for exact configuration","status":"open","priority":3,"issue_type":"feature","owner":"zach.bernstein@smarterdx.com","created_at":"2026-01-17T14:10:59.560222-06:00","created_by":"Zach Bernstein","updated_at":"2026-01-17T14:10:59.560222-06:00"}
{"id":"homelab-2e5","title":"Upgrade PostgreSQL to latest major version","description":"Upgrade the CloudNativePG (CNPG) PostgreSQL cluster from v16 to v17 seamlessly.\n\n## Current State\n\n- **Cluster name:** redspot (3 instances)\n- **Current image:** ghcr.io/cloudnative-pg/postgresql:${POSTGRESQL_VERSION}\n- **Current version:** 17.2-27 (image tag), but serverName is `postgres-v16-5`\n- **Storage:** 20Gi on openebs-hostpath\n- **Backup:** Barman to Backblaze B2 (s3://cluster-db-backup/)\n- **Bootstrap:** Recovery from `postgres-v16-4`\n\nThe serverName convention suggests the cluster was originally on PG 16 and has been through multiple recovery cycles. Need to verify if the actual running version is 16 or 17 already.\n\n## CNPG Major Version Upgrade Strategy\n\nCloudNativePG supports in-place major version upgrades via `pg_upgrade`. The process:\n\n1. Take a fresh backup (ScheduledBackup or on-demand)\n2. Increment the `serverName` (e.g., `postgres-v17-1`)\n3. Update `POSTGRESQL_VERSION` to the new major version image tag\n4. Set `bootstrap.recovery.source` to the previous serverName\n5. Update `externalClusters` to reference the previous cluster\n6. Apply -- CNPG will create a new cluster, run pg_upgrade, and promote\n\n## Steps\n\n1. [ ] Verify current running PG version (`kubectl cnpg status redspot`)\n2. [ ] Check latest stable CNPG PostgreSQL image tags\n3. [ ] Take a manual backup before upgrade\n4. [ ] Update `serverName` to next increment (e.g., `postgres-v17-1` or `postgres-v18-1`)\n5. [ ] Update `POSTGRESQL_VERSION` in ks.yaml postBuild\n6. [ ] Update `bootstrap.recovery.source` and `externalClusters`\n7. [ ] Apply and monitor upgrade\n8. [ ] Verify all dependent apps reconnect successfully\n9. [ ] Verify backups work with new version\n\n## Dependent Apps\n\nMany apps use this PostgreSQL cluster (sonarr, radarr, lidarr, prowlarr, autobrr, whisparr, gatus, atuin, homebox, mealie, nocodb, paperless, rxresume, booklore, grafana, stash, trmnl, unwrapped).\n\n## Resources\n\n- [CNPG Recovery Docs](https://cloudnative-pg.io/documentation/current/recovery/)\n- [CNPG Major Upgrades](https://cloudnative-pg.io/documentation/current/rolling_update/)","status":"open","priority":2,"issue_type":"task","owner":"zebernst@gmail.com","created_at":"2026-02-17T14:47:26.933303-06:00","created_by":"Zach Bernstein","updated_at":"2026-02-17T14:48:50.766013-06:00"}
{"id":"homelab-64x","title":"Deploy Notifier webhook service","description":"## Overview\nDeploy a centralized webhook notification service (Notifier) to handle alerts and notifications from various cluster services.\n\n## Reference\n- buroa/k8s-gitops uses Notifier for centralized webhook handling\n- GitHub: https://github.com/lrstanley/clix (or similar notification relay)\n\n## Current State\nNotifications are handled individually by each service:\n- Alertmanager → Pushover\n- Individual app notifications configured separately\n\n## Implementation\n1. Create namespace structure at kubernetes/apps/observability/notifier/ or kubernetes/apps/network/notifier/\n2. Deploy webhook relay service\n3. Configure as central notification hub:\n   - Receive webhooks from Alertmanager\n   - Receive webhooks from Flux notifications\n   - Receive webhooks from *arr apps\n   - Receive webhooks from other services\n4. Forward to notification targets:\n   - Pushover (existing)\n   - Discord (optional)\n   - Email via Maddy (after homelab-qpb)\n5. Set up ingress for webhook endpoints\n\n## Benefits\n- Centralized notification management\n- Easier to add/modify notification targets\n- Better filtering and routing of alerts\n- Reduced configuration duplication across services","status":"open","priority":3,"issue_type":"feature","owner":"zach.bernstein@smarterdx.com","created_at":"2026-01-17T14:12:48.51984-06:00","created_by":"Zach Bernstein","updated_at":"2026-01-17T14:12:48.51984-06:00"}
{"id":"homelab-6ae","title":"Migrate all ingress resources to Gateway API","description":"Migrate all ingress resources from IngressClass-based ingress to Gateway API (HTTPRoute/Gateway).\n\n## Current State\n\n### Gateways (3 total)\n- `external` - Cloudflare Tunnel (192.168.20.20), `*.zebernst.dev`\n- `internal` - LAN access (192.168.20.21), `*.zebernst.dev` + `*.internal`\n- `tailscale` - Tailscale LoadBalancer, `*.zebernst.dev`\n\n### Already Using Gateway API (16 apps)\nUsing `parentRefs` in HTTPRoute:\n- **external**: plex, wizarr, overseerr, rxresume, gatus, kromgo, echo-server, flux-webhook, rook-ceph, minecraft (dawncraft, create-integrated)\n- **internal**: glance, booklore, minecraft-router\n\n### Still Using IngressClass (27 apps - need migration)\nUsing `className: tailscale`:\n- **observability**: grafana, prometheus, karma\n- **self-hosted**: hajimari, mealie, paperless, homebox, it-tools, atuin, nocodb, radicale\n- **downloads**: All *arr apps (sonarr, radarr, lidarr, prowlarr, bazarr), qbittorrent, autobrr, etc.\n- **media**: tautulli, calibre-web, maintainerr, stash, steam\n- **ai**: local-ai\n- **rook-ceph**: dashboard (also has external route)\n- **cilium**: hubble\n\nUsing `ingressClassName: internal`:\n- loki\n\n## Target Architecture\n\n```\n                    Cloudflare Tunnel\n                          │\n                          ▼\n┌─────────────────────────────────────────────┐\n│              external Gateway               │\n│           (public *.zebernst.dev)           │\n└─────────────────────────────────────────────┘\n                          │\n          ┌───────────────┼───────────────┐\n          ▼               ▼               ▼\n       plex           wizarr          overseerr\n                                         ...\n\n┌─────────────────────────────────────────────┐\n│              internal Gateway               │\n│    (LAN *.zebernst.dev + *.internal)        │\n│    + Tailscale Funnel via ts-operator       │\n└─────────────────────────────────────────────┘\n                          │\n          ┌───────────────┼───────────────┐\n          ▼               ▼               ▼\n      grafana         sonarr          mealie\n       (all internal/tailscale apps)     ...\n```\n\n**Key change**: Apps currently on dedicated `tailscale` ingress class move to `internal` Gateway, accessible via both LAN and Tailscale (via ts-operator funnel or MagicDNS).\n\n## Implementation Steps\n\n### Phase 1: Prepare Infrastructure\n1. [ ] Ensure internal Gateway has all needed listeners (`*.internal` already added)\n2. [ ] Configure Tailscale operator to funnel traffic to internal Gateway\n3. [ ] Test that internal Gateway is reachable via Tailscale\n\n### Phase 2: Migrate Apps (by namespace)\n4. [ ] **observability**: grafana, prometheus, karma, loki\n5. [ ] **self-hosted**: hajimari, mealie, paperless, homebox, it-tools, atuin, nocodb, radicale\n6. [ ] **downloads**: sonarr, radarr, lidarr, prowlarr, bazarr, qbittorrent, autobrr, whisparr, openbooks\n7. [ ] **media**: tautulli, calibre-web, maintainerr, stash, steam\n8. [ ] **ai**: local-ai, ollama\n9. [ ] **rook-ceph**: dashboard (keep external route, add internal)\n10. [ ] **kube-system**: cilium hubble\n\n### Phase 3: Cleanup\n11. [ ] Remove dedicated tailscale Gateway (`cilium/gateway/tailscale.yaml`)\n12. [ ] Remove ingress-nginx if no longer used\n13. [ ] Update documentation\n\n## Migration Pattern\n\n**Before (IngressClass):**\n```yaml\ningress:\n  main:\n    className: tailscale\n    hosts:\n      - host: app.kite-harmonic.ts.net\n```\n\n**After (HTTPRoute):**\n```yaml\nroute:\n  main:\n    parentRefs:\n      - name: internal\n        namespace: kube-system\n    hostnames:\n      - app.zebernst.dev\n```\n\n## Blocked By\n- May need TinyCA (homelab-6rb) for internal HTTPS if using `*.internal` hostnames\n\n## Blocks\n- homelab-6rb (TinyCA) - uses Gateway API HTTP-01 solver\n- homelab-nsp (Authentik) - auth integration needs gateway","status":"open","priority":2,"issue_type":"feature","owner":"zach.bernstein@smarterdx.com","created_at":"2026-01-17T13:38:48.351989-06:00","created_by":"Zach Bernstein","updated_at":"2026-01-17T13:43:18.053659-06:00","dependencies":[{"issue_id":"homelab-6ae","depends_on_id":"homelab-0q3","type":"blocks","created_at":"2026-01-17T13:53:11.133016-06:00","created_by":"Zach Bernstein"}]}
{"id":"homelab-6rb","title":"Internal TLS with TinyCA + Gateway API","description":"Set up split certificate issuance: Let's Encrypt for public services (*.zebernst.dev) and internal TinyCA ACME server for internal services (*.internal).\n\n## Current State\n\n**Already done (uncommitted):**\n- `cert-manager/app/helm-values.yaml`: Added `--enable-gateway-api` flag\n- `cert-manager/issuers/clusterissuer.yaml` → renamed to `letsencrypt.yaml`\n- `cert-manager/issuers/tinyca.yaml`: New ClusterIssuer for internal CA (HTTP-01 via Gateway API)\n- `kube-system/cilium/gateway/internal.yaml`: Added `*.internal` HTTP listener\n\n**Not yet wired:**\n- `tinyca.yaml` not added to `issuers/kustomization.yaml`\n- `step-issuer/` directory exists but is empty (unused)\n\n## Architecture\n\n```\nPublic (*.zebernst.dev)     Internal (*.internal)\n        │                           │\n        ▼                           ▼\n  Let's Encrypt              TinyCA ACME\n  (DNS-01 via CF)          (HTTP-01 via Gateway)\n        │                           │\n        └───────────┬───────────────┘\n                    ▼\n              cert-manager\n                    │\n                    ▼\n              Certificates\n```\n\n## Implementation Steps\n\n1. [ ] Wire `tinyca.yaml` into `issuers/kustomization.yaml`\n2. [ ] Verify TinyCA is accessible at `https://tinyca.internal/acme/acme/directory`\n3. [ ] Test certificate issuance for an internal service\n4. [ ] Create a test Certificate resource targeting `tinyca-production` issuer\n5. [ ] Document the two-issuer model in CLAUDE.md or README\n6. [ ] Clean up empty `step-issuer/` directory if not needed\n\n## Files Changed\n- `kubernetes/apps/cert-manager/cert-manager/app/helm-values.yaml`\n- `kubernetes/apps/cert-manager/cert-manager/issuers/kustomization.yaml`\n- `kubernetes/apps/cert-manager/cert-manager/issuers/letsencrypt.yaml` (renamed)\n- `kubernetes/apps/cert-manager/cert-manager/issuers/tinyca.yaml` (new)\n- `kubernetes/apps/kube-system/cilium/gateway/internal.yaml`\n\n## Dependencies\n- Blocked by: homelab-6ae (Gateway API migration) - uses gatewayHTTPRoute solver","status":"open","priority":2,"issue_type":"feature","owner":"zach.bernstein@smarterdx.com","created_at":"2026-01-17T13:26:01.096164-06:00","created_by":"Zach Bernstein","updated_at":"2026-01-17T13:40:39.723588-06:00","dependencies":[{"issue_id":"homelab-6rb","depends_on_id":"homelab-6ae","type":"blocks","created_at":"2026-01-17T13:39:59.121176-06:00","created_by":"Zach Bernstein"}]}
{"id":"homelab-86c","title":"Migrate from Loki to Victoria Logs","description":"Migrate log aggregation from Loki to Victoria Logs for lower resource usage and simpler operation.\n\n## Current Setup (Your Repo)\n\n```yaml\n# kubernetes/apps/observability/loki/app/helmrelease.yaml\ndeploymentMode: SingleBinary\nloki:\n  storage:\n    type: filesystem\n  limits_config:\n    retention_period: 14d\nsingleBinary:\n  persistence:\n    storageClass: ceph-block\n    size: 50Gi\n```\n\n- **Chart**: grafana/loki v6.40.0\n- **Mode**: SingleBinary\n- **Storage**: 50Gi Ceph block\n- **Retention**: 14 days\n\n## Reference Setup (buroa \u0026 onedr0p)\n\nBoth repos have migrated to **Victoria Logs**:\n\n```yaml\n# Victoria Logs configuration\nretention: 14d\nstorage: 20Gi  # Much smaller!\n```\n\n- **Chart**: victoria-metrics/victoria-logs\n- **Storage**: 20Gi (60% smaller)\n- **WebUI**: Built-in at victoria-logs.domain.com\n\n## Why Migrate?\n\n1. **Lower Resource Usage**: Victoria Logs uses ~60% less storage for same retention\n2. **Simpler Architecture**: Single binary, no complex microservices\n3. **Better Performance**: Faster queries, especially for high-cardinality logs\n4. **Native LogsQL**: Powerful query language\n5. **Drop-in Replacement**: Compatible with Loki push API\n\n## Implementation Steps\n\n1. [ ] Deploy Victoria Logs alongside Loki\n2. [ ] Configure Fluent Bit (or Promtail) to send to both\n3. [ ] Test Victoria Logs queries and WebUI\n4. [ ] Migrate Grafana dashboards to use Victoria Logs datasource\n5. [ ] Verify 14-day retention working correctly\n6. [ ] Remove Loki deployment\n7. [ ] Update Grafana to use Victoria Logs as primary\n\n## Files to Create\n\n```\nkubernetes/apps/observability/victoria-logs/\n├── ks.yaml\n└── app/\n    ├── kustomization.yaml\n    └── helmrelease.yaml\n```\n\n## Helm Chart\n\n```yaml\nrepository: https://victoriametrics.github.io/helm-charts\nchart: victoria-logs-single\n```\n\n## Related Issues\n\n- Should be done alongside Fluent Bit migration for optimal setup\n\n## Resources\n\n- [Victoria Logs Docs](https://docs.victoriametrics.com/victorialogs/)\n- [buroa config](https://github.com/buroa/k8s-gitops/tree/main/kubernetes/apps/observability)\n- [onedr0p config](https://github.com/onedr0p/home-ops/tree/main/kubernetes/apps/observability)","status":"open","priority":3,"issue_type":"feature","owner":"zach.bernstein@smarterdx.com","created_at":"2026-01-17T14:09:41.912975-06:00","created_by":"Zach Bernstein","updated_at":"2026-01-17T14:09:41.912975-06:00"}
{"id":"homelab-92p","title":"Full Tailscale Kubernetes Operator integration","description":"## Overview\nEnhance the current Tailscale Kubernetes Operator deployment to leverage its full capabilities, improving availability, security, and operational visibility.\n\n## Current Implementation\n- Operator v1.86.2 with OAuth credentials from 1Password\n- ProxyClass `tailscale-tun` with TUN device access and NET_ADMIN\n- API server proxy enabled (mode: true)\n- Ingress class `tailscale` used by 25+ apps\n- LoadBalancer pattern for Plex, Calibre-Web\n- RBAC ClusterRoleBinding for `zebernst@gmail.com` as cluster-admin\n- Cilium gateway integration (`cilium-ts` class with `loadBalancerClass: tailscale`)\n- MagicDNS domain: `kite-harmonic.ts.net`\n\n## Improvements\n\n### 1. High Availability with ProxyGroups (P1)\n**Gap:** Single proxy pods - no redundancy during upgrades/restarts\n**Solution:** Deploy ProxyGroup resources for ingress\n```yaml\napiVersion: tailscale.com/v1alpha1\nkind: ProxyGroup\nmetadata:\n  name: ingress-proxies\nspec:\n  type: ingress\n  replicas: 2\n```\n- Eliminates downtime during pod restarts\n- Better resource utilization\n- Reference: buroa/k8s-gitops uses ProxyGroups\n\n### 2. Session Recording with Recorder CRD (P2)\n**Gap:** No audit logging for kubectl sessions via API server proxy\n**Solution:** Deploy Recorder for compliance and security auditing\n```yaml\napiVersion: tailscale.com/v1alpha1\nkind: Recorder\nmetadata:\n  name: kubectl-recorder\nspec:\n  enableUI: true\n  # Configure S3 for persistent storage (Backblaze B2 compatible)\n```\n- Records kubectl exec/attach/debug sessions\n- Web UI for reviewing recordings\n- Integrate with existing B2 backup infrastructure\n\n### 3. Metrics and Observability (P2)\n**Gap:** ProxyClass doesn't enable metrics\n**Solution:** Enable metrics in ProxyClass for Prometheus scraping\n```yaml\napiVersion: tailscale.com/v1alpha1\nkind: ProxyClass\nmetadata:\n  name: tailscale-tun\nspec:\n  metrics:\n    enable: true\n```\n- Integrate with existing kube-prometheus-stack\n- Create Grafana dashboard for Tailscale proxy metrics\n\n## Implementation Order\n1. **ProxyGroups** - Immediate HA improvement\n2. **Metrics** - Quick win for observability  \n3. **Recorder** - Security/compliance enhancement\n\n## Notes\n- Tailscale operator supports Kubernetes Ingress, NOT Gateway API HTTPRoute natively\n- Current Cilium gateway + `loadBalancerClass: tailscale` is the correct approach for Gateway API + Tailscale\n\n## Reference\n- Docs: https://tailscale.com/kb/1236/kubernetes-operator\n- buroa/k8s-gitops uses ProxyGroups and advanced Tailscale patterns","status":"open","priority":2,"issue_type":"feature","owner":"zach.bernstein@smarterdx.com","created_at":"2026-01-18T13:37:03.711615-06:00","created_by":"Zach Bernstein","updated_at":"2026-01-18T13:42:54.886867-06:00"}
{"id":"homelab-aj6","title":"Evaluate Volsync perfectra1n fork","description":"Evaluate whether to switch from standard Volsync to the perfectra1n fork used by reference repos.\n\n## Current Setup (Your Repo)\n\nUsing standard Volsync from backube:\n\n```yaml\n# kubernetes/apps/volsync-system/volsync/\n# Standard Volsync deployment\n```\n\n## Reference Setup (onedr0p)\n\nUses **perfectra1n fork** of Volsync:\n\n```yaml\nimage:\n  repository: ghcr.io/perfectra1n/volsync\n  tag: 0.17.6\n```\n\nFeatures of the fork:\n- **Multiple backup tools**: Kopia, Rclone, Restic, Rsync, Rsync-TLS, Syncthing\n- **Enhanced security**: Non-root UID 1000, read-only considerations\n- **CRD Management**: Automatic CRD updates\n- **Bug fixes**: Community patches not yet in upstream\n\n## Investigation Needed\n\n1. [ ] Research what the perfectra1n fork adds over upstream\n2. [ ] Check GitHub for fork differences/commits\n3. [ ] Evaluate if the additional features are useful\n4. [ ] Test compatibility with existing ReplicationSources\n5. [ ] Decide: migrate or stay with upstream\n\n## Questions to Answer\n\n- What specific features does the fork add?\n- Is it actively maintained?\n- Are there breaking changes from upstream?\n- Will existing backup configurations continue to work?\n\n## Resources\n\n- [perfectra1n/volsync](https://github.com/perfectra1n/volsync)\n- [backube/volsync (upstream)](https://github.com/backube/volsync)\n- Compare commits between fork and upstream","status":"open","priority":3,"issue_type":"chore","owner":"zach.bernstein@smarterdx.com","created_at":"2026-01-17T14:11:11.192588-06:00","created_by":"Zach Bernstein","updated_at":"2026-01-17T14:11:11.192588-06:00"}
{"id":"homelab-cbn","title":"Evaluate migration from Taskfile to Justfile","description":"Evaluate and potentially migrate from Taskfile (YAML) to Justfile (Make-like DSL) for task automation.\n\n## Background\n\nonedr0p/home-ops has migrated to Justfile while buroa/k8s-gitops still uses Taskfile. This is a low-priority exploration to determine if migration is worthwhile.\n\n## Current Setup\n\n- `Taskfile.yaml` at root\n- Modules in `.taskfiles/` (bootstrap, kubernetes, synology, talos, volsync)\n- ~25 tasks total\n- Uses: preconditions, prompts, variable interpolation, dotenv\n\n## Comparison Summary\n\n| Feature | Taskfile (Current) | Just |\n|---------|-------------------|------|\n| Syntax | YAML | Make-like DSL |\n| Parameters | `VAR=value` | Positional args |\n| Preconditions | Built-in | Manual shell checks |\n| Prompts | Built-in | Requires `gum` |\n| Schema validation | Yes | No |\n| Multi-language recipes | Limited | Excellent |\n| Startup time | ~10-20ms | ~2-3ms |\n\n## Benefits of Just\n\n1. Cleaner CLI: `just deploy prod` vs `task deploy ENV=prod`\n2. Multi-language recipes (Python, Node, etc.)\n3. Simpler syntax for shell-heavy tasks\n4. Built-in shell completions\n5. Faster startup (Rust vs Go)\n\n## Drawbacks of Just\n\n1. Loss of declarative `preconditions:`\n2. Loss of built-in `prompt:` (need `gum`)\n3. No JSON Schema for IDE validation\n4. No checksum-based task skipping\n5. Learning curve for team\n6. Migration effort (~25 tasks)\n\n## Implementation Steps (if proceeding)\n\n### Phase 1: Evaluation\n1. [ ] Install `just` via mise\n2. [ ] Create proof-of-concept `.justfile` with 2-3 tasks\n3. [ ] Test ergonomics and identify pain points\n4. [ ] Decide go/no-go\n\n### Phase 2: Migration (if approved)\n5. [ ] Create root `.justfile` with shell config and helpers\n6. [ ] Create `bootstrap/mod.just`\n7. [ ] Create `kubernetes/mod.just`\n8. [ ] Create `talos/mod.just`\n9. [ ] Create `volsync/mod.just` (if needed)\n10. [ ] Add `gum` to mise for interactive prompts\n11. [ ] Test all recipes\n12. [ ] Remove old `.taskfiles/` and `Taskfile.yaml`\n13. [ ] Update CLAUDE.md documentation\n\n## Reference Implementation\n\nonedr0p/home-ops structure:\n```\n.justfile              # Root with imports\nbootstrap/mod.just     # Bootstrap recipes\nkubernetes/mod.just    # K8s recipes  \ntalos/mod.just         # Talos recipes\n```\n\nKey patterns:\n- `[private]` attribute for helper recipes\n- `gum log` for structured logging\n- `gum confirm` for prompts\n\n## Resources\n\n- [Just Manual](https://just.systems/man/en/)\n- [onedr0p/home-ops .justfile](https://github.com/onedr0p/home-ops/blob/main/.justfile)\n- [Taskfile vs Justfile comparison](https://nguyenhuythanh.com/posts/taskfile-vs-justfile/)","status":"open","priority":3,"issue_type":"chore","owner":"zach.bernstein@smarterdx.com","created_at":"2026-01-17T13:57:59.349511-06:00","created_by":"Zach Bernstein","updated_at":"2026-01-17T13:57:59.349511-06:00"}
{"id":"homelab-fja","title":"Deploy Actions Runner Controller for self-hosted CI/CD","description":"Deploy GitHub Actions Runner Controller (ARC) to run CI/CD workloads in the cluster.\n\n## Current Setup (Your Repo)\n\nNo self-hosted GitHub Actions runners. CI/CD runs on GitHub-hosted runners.\n\n## Reference Setup (buroa \u0026 onedr0p)\n\nBoth repos use **Actions Runner Controller**:\n\n```yaml\n# Namespace: actions-runner-system\n# Components:\n- actions-runner-controller (operator)\n- runner-scale-sets (dynamic runners)\n```\n\nFeatures:\n- Kubernetes-native runner scaling\n- Dynamic capacity based on workflow demand\n- Cost savings vs GitHub-hosted runners\n- Access to cluster resources during CI\n\n## Why Deploy?\n\n1. **Cost Savings**: Free compute vs GitHub-hosted runner minutes\n2. **Speed**: No queue time, runners already warm\n3. **Access**: Runners can interact with cluster directly\n4. **Customization**: Install any tools/dependencies\n5. **Security**: Workflows run in your infrastructure\n\n## Implementation Steps\n\n1. [ ] Create `actions-runner-system` namespace\n2. [ ] Deploy ARC operator via Helm\n3. [ ] Configure runner scale set for the repo\n4. [ ] Set up GitHub App or PAT for authentication\n5. [ ] Store credentials in 1Password/external-secrets\n6. [ ] Test with a simple workflow\n7. [ ] Migrate existing workflows to use self-hosted runners\n\n## Files to Create\n\n```\nkubernetes/apps/actions-runner-system/\n├── kustomization.yaml\n├── actions-runner-controller/\n│   ├── ks.yaml\n│   └── app/\n│       ├── kustomization.yaml\n│       ├── helmrelease.yaml\n│       └── externalsecret.yaml\n└── runners/\n    ├── ks.yaml\n    └── app/\n        ├── kustomization.yaml\n        └── helmrelease.yaml\n```\n\n## Helm Charts\n\n```yaml\n# Controller\nrepository: oci://ghcr.io/actions/actions-runner-controller-charts\nchart: gha-runner-scale-set-controller\n\n# Runner Scale Set\nrepository: oci://ghcr.io/actions/actions-runner-controller-charts\nchart: gha-runner-scale-set\n```\n\n## Configuration Example\n\n```yaml\n# Runner scale set\ngithubConfigUrl: https://github.com/zebernst/homelab\ngithubConfigSecret: github-runner-secret\nminRunners: 0\nmaxRunners: 5\n```\n\n## Resources\n\n- [ARC Documentation](https://github.com/actions/actions-runner-controller)\n- [ARC Helm Charts](https://github.com/actions/actions-runner-controller/tree/master/charts)","status":"open","priority":3,"issue_type":"feature","owner":"zach.bernstein@smarterdx.com","created_at":"2026-01-17T14:10:24.758831-06:00","created_by":"Zach Bernstein","updated_at":"2026-01-17T14:10:24.758831-06:00"}
{"id":"homelab-ngi","title":"Evaluate Tuppr vs system-upgrade-controller","description":"Evaluate whether to migrate from system-upgrade-controller to Tuppr for Talos/Kubernetes upgrades.\n\n## Current Setup (Your Repo)\n\n```\nkubernetes/apps/system-upgrade/\n├── system-upgrade-controller/\n└── versions.env\n```\n\nUsing standard **system-upgrade-controller** for coordinated node upgrades.\n\n## Reference Setup (buroa \u0026 onedr0p)\n\nBoth repos use **Tuppr**:\n\n```\nkubernetes/apps/system-upgrade/\n├── tuppr/\n│   ├── app/\n│   └── upgrades/\n```\n\nTuppr appears to be a wrapper/alternative that provides:\n- App deployment configuration\n- Separate upgrades configuration\n- Potentially more features for Talos-specific upgrades\n\n## Investigation Needed\n\n1. [ ] Research what Tuppr actually is and how it differs\n2. [ ] Check if Tuppr is a community project or custom solution\n3. [ ] Compare feature sets:\n   - Rolling upgrade support\n   - Drain/cordon behavior\n   - Talos-specific integration\n   - Plan approval workflow\n4. [ ] Evaluate migration complexity\n5. [ ] Decide: migrate or stay with system-upgrade-controller\n\n## Questions to Answer\n\n- Is Tuppr a Helm chart or custom manifests?\n- Does it provide better Talos Linux integration?\n- What's the upgrade coordination model?\n- Is there a web UI or CLI for managing upgrades?\n\n## Resources\n\n- [system-upgrade-controller](https://github.com/rancher/system-upgrade-controller)\n- Check buroa/onedr0p repos for Tuppr source/docs","status":"open","priority":3,"issue_type":"chore","owner":"zach.bernstein@smarterdx.com","created_at":"2026-01-17T14:10:10.216284-06:00","created_by":"Zach Bernstein","updated_at":"2026-01-17T14:10:10.216284-06:00"}
{"id":"homelab-nsp","title":"Add Authentik as auth provider","description":"Deploy Authentik as an identity provider to replace Tailscale identity headers and provide SSO for applications.\n\n## Background\n\nCurrently using Tailscale Ingress headers for authentication:\n- Only Grafana uses `Tailscale-User-Login` / `Tailscale-User-Name` headers\n- Moving away from dedicated Tailscale loadbalancer means losing these headers\n- Need a proper identity provider for internal apps\n\n## Authentik Features to Leverage\n- Forward auth proxy (for apps without native OIDC)\n- OIDC/OAuth2 provider (for apps with native support)\n- LDAP provider (for apps requiring LDAP)\n- SSO across all internal applications\n- User/group management\n\n## Current Auth State\n\n| App | Current Auth | Target Auth |\n|-----|--------------|-------------|\n| Grafana | Tailscale headers | Authentik proxy auth |\n| Prometheus | None (network) | Authentik proxy auth |\n| Most *arr apps | Built-in | Keep built-in (optional SSO) |\n| Plex | Plex auth | Keep (optional Authentik) |\n| Overseerr | Plex OAuth | Keep (optional Authentik) |\n\n## Implementation Steps\n\n### Phase 1: Deploy Authentik\n1. [ ] Create `security` namespace (or use `self-hosted`)\n2. [ ] Deploy PostgreSQL for Authentik (via CNPG or embedded)\n3. [ ] Deploy Redis for Authentik (Dragonfly instance or embedded)\n4. [ ] Create HelmRelease for Authentik\n5. [ ] Configure external-secrets for Authentik bootstrap credentials\n6. [ ] Expose via internal Gateway (`auth.zebernst.dev` or `authentik.internal`)\n\n### Phase 2: Configure Base Setup\n7. [ ] Initial admin setup and password change\n8. [ ] Configure email notifications (requires homelab-qpb Maddy first, or external SMTP)\n9. [ ] Set up branding/theming\n10. [ ] Create default authentication flow\n\n### Phase 3: Integrate Applications\n11. [ ] **Grafana**: Migrate from Tailscale headers to Authentik proxy auth\n12. [ ] **Prometheus**: Add Authentik forward auth\n13. [ ] **Karma**: Add Authentik forward auth\n14. [ ] (Optional) Configure OIDC for apps with native support\n\n### Phase 4: Forward Auth Setup\n15. [ ] Create outpost for forward auth proxy\n16. [ ] Configure Cilium Gateway to use forward auth\n17. [ ] Test authentication flow end-to-end\n\n## Files to Create\n- `kubernetes/apps/security/authentik/ks.yaml`\n- `kubernetes/apps/security/authentik/app/helmrelease.yaml`\n- `kubernetes/apps/security/authentik/app/externalsecret.yaml`\n- `kubernetes/apps/security/authentik/app/kustomization.yaml`\n- `kubernetes/apps/security/kustomization.yaml`\n\n## Dependencies\n- **Blocked by**: homelab-6ae (Gateway API migration) - need gateway for forward auth integration\n- **Related**: homelab-qpb (Maddy SMTP) - for email notifications\n\n## Helm Chart\n```yaml\n# goauthentik/authentik\nrepository: https://charts.goauthentik.io\nchart: authentik\n```\n\n## Resources\n- [Authentik Docs](https://goauthentik.io/docs/)\n- [Forward Auth Proxy](https://goauthentik.io/docs/providers/proxy/forward_auth)\n- [OIDC Provider](https://goauthentik.io/docs/providers/oauth2/)","status":"open","priority":2,"issue_type":"feature","owner":"zach.bernstein@smarterdx.com","created_at":"2026-01-17T13:38:48.552175-06:00","created_by":"Zach Bernstein","updated_at":"2026-01-17T13:44:04.774939-06:00","dependencies":[{"issue_id":"homelab-nsp","depends_on_id":"homelab-6ae","type":"blocks","created_at":"2026-01-17T13:39:59.048132-06:00","created_by":"Zach Bernstein"}]}
{"id":"homelab-plv","title":"Deploy Agregarr for media request aggregation","description":"## Overview\nDeploy Agregarr to aggregate media request sources into a unified interface.\n\n## Reference\n- buroa/k8s-gitops uses Agregarr for media request management\n- GitHub: https://github.com/rotkonetmedia/agregarr\n\n## Current State\nNo centralized media request aggregation - users interact directly with individual *arr apps.\n\n## Implementation\n1. Create namespace structure at kubernetes/apps/media/agregarr/\n2. Deploy HelmRelease using bjw-s app-template\n3. Configure integration with existing *arr stack:\n   - Sonarr (HD and UHD instances)\n   - Radarr (HD and UHD instances)\n   - Possibly Jellyseerr/Overseerr\n4. Set up ingress via Gateway API (internal + tailscale)\n5. Configure persistence for settings\n\n## Benefits\n- Unified interface for media requests\n- Better visibility into request status across services\n- Simplified user experience for media management","status":"open","priority":3,"issue_type":"feature","owner":"zach.bernstein@smarterdx.com","created_at":"2026-01-17T14:12:48.3399-06:00","created_by":"Zach Bernstein","updated_at":"2026-01-17T14:12:48.3399-06:00"}
{"id":"homelab-pm9","title":"Standardize Flux/Kustomize resource conventions","description":"Standardize differences amongst the Flux/Kustomize resources across the codebase for consistency and maintainability.\n\n## Current State Analysis\n\n**83 ks.yaml files** with the following inconsistencies:\n\n### Schema Comments\n- 69/83 have `yaml-language-server` schema comment\n- 14 missing schema comments\n\n### Namespace Aliases\n- Most use `\u0026ns` (87 occurrences)\n- Some use `\u0026namespace` (10 occurrences)\n- Should standardize on `\u0026ns`\n\n### wait: Setting\n- 72 have `wait: false`\n- 28 have `wait: true`\n- Need to determine correct policy per resource type\n\n### interval: Values\n- 90 use `30m`\n- 12 use `1h`\n- Should standardize (likely `30m` for most)\n\n### retryInterval:\n- 82/83 have `retryInterval`\n- 1 missing\n\n### prune: Setting\n- 7 have `prune: false` (intentional for critical resources like flux-operator)\n- Rest have `prune: true`\n\n### Key Ordering\nVaries across files. Proposed standard order:\n```yaml\nspec:\n  targetNamespace: *ns\n  commonMetadata:\n    labels: ...\n  dependsOn: ...\n  path: ...\n  components: ...    # if used\n  postBuild: ...     # if used\n  healthCheckExprs:  # if used\n  prune: true\n  sourceRef: ...\n  wait: false\n  interval: 30m\n  retryInterval: 1m\n  timeout: 5m\n```\n\n## Implementation Steps\n\n### Phase 1: Define Standards\n1. [ ] Document the canonical ks.yaml structure\n2. [ ] Determine wait: policy (false for apps, true for operators/CRDs?)\n3. [ ] Decide on interval (30m standard, 1h for slow-changing?)\n\n### Phase 2: Automated Fixes\n4. [ ] Add missing yaml-language-server schema comments\n5. [ ] Standardize namespace alias `\u0026namespace` → `\u0026ns`\n6. [ ] Add missing retryInterval fields\n7. [ ] Normalize interval values\n\n### Phase 3: Manual Review\n8. [ ] Review key ordering in each file\n9. [ ] Verify wait: values are intentional\n10. [ ] Check prune: false is only on critical resources\n\n### Phase 4: Tooling\n11. [ ] Consider adding a linter/formatter (e.g., yamlfmt with custom config)\n12. [ ] Add pre-commit hook to enforce standards\n\n## Files Affected\n- All 83 `kubernetes/apps/**/ks.yaml` files\n- Potentially `kubernetes/apps/**/kustomization.yaml` files too","status":"in_progress","priority":2,"issue_type":"chore","owner":"zach.bernstein@smarterdx.com","created_at":"2026-01-17T13:38:48.148411-06:00","created_by":"Zach Bernstein","updated_at":"2026-02-17T14:45:46.885607-06:00"}
{"id":"homelab-qpb","title":"Set up noreply/SMTP relay with Maddy","description":"Deploy Maddy as an SMTP relay for sending outbound emails from cluster applications.\n\n## Background\n\nCurrently no internal SMTP relay exists. Apps requiring email:\n- **rxresume**: Has SMTP_URL commented out, uses `noreply@zebernst.dev`\n- **Authentik** (future): Will need email for notifications, password resets\n- **Grafana**: Alert notifications (currently likely disabled)\n- **Other apps**: Various notification needs\n\n## Maddy Overview\n\nMaddy is a composable all-in-one mail server that can act as:\n- SMTP relay (outbound only - our use case)\n- Full mail server (not needed)\n\nFor our use case, configure as **outbound-only relay** that:\n1. Accepts mail from cluster pods\n2. Relays via upstream provider (e.g., Mailgun, SendGrid, or direct)\n3. Signs with DKIM\n\n## Implementation Steps\n\n### Phase 1: DNS Setup\n1. [ ] Decide on sending domain: `zebernst.dev` or subdomain like `mail.zebernst.dev`\n2. [ ] Create SPF record: `v=spf1 include:_spf.mx.cloudflare.net ~all` (or relay provider)\n3. [ ] Generate DKIM keys and add DNS record\n4. [ ] Create DMARC record: `v=DMARC1; p=quarantine; rua=mailto:dmarc@zebernst.dev`\n\n### Phase 2: Deploy Maddy\n5. [ ] Create `network/maddy/` directory structure\n6. [ ] Create HelmRelease or raw manifests for Maddy\n7. [ ] Configure as outbound-only relay\n8. [ ] Store DKIM private key in 1Password/external-secrets\n9. [ ] Expose on internal ClusterIP (smtp.network.svc.cluster.local:25/587)\n\n### Phase 3: Configuration\n10. [ ] Configure upstream relay (direct, Mailgun, SendGrid, etc.)\n11. [ ] Set up sender address restrictions (only `*@zebernst.dev`)\n12. [ ] Configure TLS for STARTTLS support\n13. [ ] Set up logging/monitoring\n\n### Phase 4: Integrate Applications\n14. [ ] **rxresume**: Uncomment and configure SMTP_URL\n15. [ ] **Authentik**: Configure SMTP settings (when deployed)\n16. [ ] **Grafana**: Configure SMTP for alert notifications\n17. [ ] Document SMTP endpoint for future apps\n\n## Maddy Configuration Example\n\n```\n# /etc/maddy/maddy.conf\nsmtp tcp://0.0.0.0:25 tcp://0.0.0.0:587 {\n    limits {\n        all rate 20 1s\n    }\n\n    dkim zebernst.dev {\n        key_path /secrets/dkim.key\n        selector mail\n        domain zebernst.dev\n    }\n\n    deliver_to \u0026remote_queue\n}\n\ntarget.queue remote_queue {\n    target \u0026outbound_delivery\n}\n\ntarget.smtp outbound_delivery {\n    # Direct delivery or relay config\n}\n```\n\n## Files to Create\n- `kubernetes/apps/network/maddy/ks.yaml`\n- `kubernetes/apps/network/maddy/app/helmrelease.yaml` (or deployment.yaml)\n- `kubernetes/apps/network/maddy/app/configmap.yaml`\n- `kubernetes/apps/network/maddy/app/externalsecret.yaml`\n- `kubernetes/apps/network/maddy/app/kustomization.yaml`\n\n## Service Endpoint\n```\nsmtp.network.svc.cluster.local:25   # SMTP\nsmtp.network.svc.cluster.local:587  # Submission (STARTTLS)\n```\n\n## Related Issues\n- homelab-nsp (Authentik) - needs email for notifications\n\n## Resources\n- [Maddy Docs](https://maddy.email/)\n- [Maddy Docker Image](https://hub.docker.com/r/foxcpp/maddy)\n- [Outbound-only setup guide](https://maddy.email/tutorials/smtp-relay/)","status":"in_progress","priority":2,"issue_type":"feature","owner":"zach.bernstein@smarterdx.com","created_at":"2026-01-17T13:38:48.753088-06:00","created_by":"Zach Bernstein","updated_at":"2026-02-17T14:45:46.962916-06:00"}
{"id":"homelab-smo","title":"Migrate deprecated Cilium BGP CRDs to v2 API","description":"Migrate deprecated Cilium v2alpha1 CRDs to stable v2 API before upgrading to Cilium 1.19.0+.\n\n## Plan\n\nSee `docs/plans/2026-02-17-cilium-bgp-crd-migration.md` for full implementation plan.\n\n## Current State\n\n- **Cilium version:** 1.18.0\n- **CRDs on v2alpha1 (need migration):**\n  - `CiliumBGPClusterConfig` (l3.yaml) -- BGP peering with UDM Pro (ASN 64512 \u003c-\u003e 64513)\n  - `CiliumBGPPeerConfig` (l3.yaml) -- graceful restart, IPv4+IPv6 unicast\n  - `CiliumBGPAdvertisement` (l3.yaml) -- PodCIDR + Service advertisements\n  - `CiliumLoadBalancerIPPool` (pool.yaml) -- 192.168.20.0/24 IP pool\n- **Already on v2:** `CiliumNodeConfig` (node-europa.yaml)\n- **Stays on v2alpha1:** `CiliumL2AnnouncementPolicy` (l2.yaml) -- no v2 available yet\n\n## Migration\n\nAll 4 CRDs require only an `apiVersion` bump from `cilium.io/v2alpha1` to `cilium.io/v2`. No spec/schema changes needed -- the v2 API is structurally identical.\n\n## Why\n\n- Cilium 1.18.0 deprecated v2alpha1 for BGP CRDs\n- Cilium 1.19.0 removes `CiliumBGPPeeringPolicy` entirely (we don't use this, but signals v2alpha1 removal is coming)\n- v2alpha1 will be removed in a future release (1.20 or 1.21)\n\n## Files Changed\n\n- `kubernetes/apps/kube-system/cilium/config/l3.yaml` (3 apiVersion changes)\n- `kubernetes/apps/kube-system/cilium/config/pool.yaml` (1 apiVersion change)\n\n## Resources\n\n- [Cilium 1.18.0 Release Notes](https://github.com/cilium/cilium/releases/tag/v1.18.0)\n- [Cilium Upgrade Guide](https://docs.cilium.io/en/stable/operations/upgrade/)\n- [BGP Control Plane v2 docs](https://docs.cilium.io/en/stable/network/bgp-control-plane/bgp-control-plane-v2/)","status":"open","priority":2,"issue_type":"task","owner":"zebernst@gmail.com","created_at":"2026-02-17T17:57:25.764362-06:00","created_by":"Zach Bernstein","updated_at":"2026-02-17T18:12:23.939206-06:00"}
{"id":"homelab-v14","title":"Migrate from Promtail to Fluent Bit","description":"Migrate log collection from Promtail to Fluent Bit for better efficiency and flexibility.\n\n## Current Setup (Your Repo)\n\n```yaml\n# kubernetes/apps/observability/promtail/app/helmrelease.yaml\nchart: promtail\nversion: 6.17.0\nvalues:\n  config:\n    clients:\n      - url: http://loki-headless.observability.svc.cluster.local:3100/loki/api/v1/push\n```\n\n- **Chart**: grafana/promtail v6.17.0\n- **Output**: Loki only\n- **Features**: Basic Kubernetes metadata\n\n## Reference Setup (buroa \u0026 onedr0p)\n\nBoth repos use **Fluent Bit**:\n\n```yaml\n# Fluent Bit configuration highlights\nfilters:\n  - kubernetes (pod metadata enrichment)\n  - nest (field flattening)\noutput:\n  - victoria-logs (gzip compressed)\n```\n\n- **Chart**: fluent/fluent-bit\n- **Features**: \n  - Kubernetes filter for pod metadata\n  - Nest filters for field flattening  \n  - Gzip compression for payloads\n  - Lower memory footprint\n\n## Why Migrate?\n\n1. **Lower Resource Usage**: Fluent Bit uses less CPU/memory than Promtail\n2. **Better Filtering**: More powerful filter plugins\n3. **Compression**: Built-in gzip reduces network/storage\n4. **Multi-Output**: Can send to multiple backends simultaneously\n5. **Pairs with Victoria Logs**: Optimal combination\n\n## Implementation Steps\n\n1. [ ] Deploy Fluent Bit alongside Promtail\n2. [ ] Configure Kubernetes metadata enrichment\n3. [ ] Set up gzip compression for outputs\n4. [ ] Configure output to Victoria Logs (or Loki during transition)\n5. [ ] Test log collection from all namespaces\n6. [ ] Verify Grafana can query logs\n7. [ ] Remove Promtail deployment\n\n## Files to Create\n\n```\nkubernetes/apps/observability/fluent-bit/\n├── ks.yaml\n└── app/\n    ├── kustomization.yaml\n    └── helmrelease.yaml\n```\n\n## Sample Configuration\n\n```yaml\nconfig:\n  filters: |\n    [FILTER]\n        Name kubernetes\n        Match kube.*\n        Merge_Log On\n        Keep_Log Off\n        K8S-Logging.Parser On\n        K8S-Logging.Exclude On\n    [FILTER]\n        Name nest\n        Match kube.*\n        Operation lift\n        Nested_under kubernetes\n        Add_prefix kubernetes_\n  outputs: |\n    [OUTPUT]\n        Name http\n        Match kube.*\n        Host victoria-logs.observability.svc.cluster.local\n        Port 9428\n        URI /insert/jsonline?_stream_fields=kubernetes_pod_name,kubernetes_namespace_name\n        Format json_lines\n        Compress gzip\n```\n\n## Related Issues\n\n- Best done alongside Victoria Logs migration (homelab-86c)\n\n## Resources\n\n- [Fluent Bit Docs](https://docs.fluentbit.io/)\n- [Fluent Bit Helm Chart](https://github.com/fluent/helm-charts)","status":"open","priority":3,"issue_type":"feature","owner":"zach.bernstein@smarterdx.com","created_at":"2026-01-17T14:09:57.488763-06:00","created_by":"Zach Bernstein","updated_at":"2026-01-17T14:09:57.488763-06:00"}
{"id":"homelab-vab","title":"Advanced Cilium tuning (BBR, Maglev, netkit eBPF)","description":"Apply advanced Cilium configuration from reference repos for improved networking performance.\n\n## Current Setup (Your Repo)\n\n```yaml\n# kubernetes/apps/kube-system/cilium/app/helmrelease.yaml\n# Minimal config - relies on ConfigMap for main values\nvalues:\n  hubble:\n    enabled: true\n    # ... hubble config\n  operator:\n    prometheus:\n      enabled: true\n  prometheus:\n    enabled: true\n```\n\n**Missing advanced features** that both reference repos have enabled.\n\n## Reference Setup (buroa \u0026 onedr0p)\n\nBoth repos have extensive Cilium tuning:\n\n```yaml\n# Key performance settings\nkubeProxyReplacement: true          # Replace kube-proxy entirely\nroutingMode: native                 # Native routing vs overlay\nbpf:\n  datapathMode: netkit              # Kernel-space eBPF datapath\n  masquerade: true\nbandwidthManager:\n  enabled: true\n  bbr: true                         # BBR congestion control\nipv4:\n  enabled: true\nenableIPv4BIGTCP: true              # Big TCP for high throughput\nloadBalancer:\n  algorithm: maglev                 # Consistent hashing LB\n  mode: dsr                         # Direct Server Return\n  acceleration: native              # Hardware acceleration\nbgpControlPlane:\n  enabled: true                     # Native BGP for routing\n```\n\n## Key Configuration Differences\n\n| Feature | Your Setup | Reference | Impact |\n|---------|------------|-----------|--------|\n| kubeProxyReplacement | Unknown | `true` | Removes kube-proxy overhead |\n| bpf.datapathMode | Default | `netkit` | Kernel-space processing |\n| bandwidthManager.bbr | Disabled | `true` | Better congestion control |\n| enableIPv4BIGTCP | Disabled | `true` | Higher throughput |\n| loadBalancer.algorithm | Default | `maglev` | Consistent hashing |\n| loadBalancer.mode | Default | `dsr` | Direct Server Return |\n| bgpControlPlane | Unknown | `true` | Dynamic BGP routing |\n\n## Why Tune?\n\n1. **kubeProxyReplacement**: Eliminates iptables overhead, pure eBPF\n2. **netkit datapath**: Processes packets in kernel space, lower latency\n3. **BBR bandwidth manager**: Google's congestion control, better throughput\n4. **BIGTCP**: Larger TCP segments for high-bandwidth workloads\n5. **Maglev LB**: Consistent hashing for stable backend selection\n6. **DSR mode**: Responses bypass load balancer, lower latency\n7. **BGP control plane**: Dynamic route advertisement\n\n## Implementation Steps\n\n1. [ ] Audit current Cilium ConfigMap values\n2. [ ] Enable `kubeProxyReplacement: true`\n3. [ ] Configure `bpf.datapathMode: netkit`\n4. [ ] Enable bandwidth manager with BBR\n5. [ ] Enable IPv4 BIGTCP\n6. [ ] Configure Maglev load balancing with DSR\n7. [ ] Enable BGP control plane (if using BGP)\n8. [ ] Apply incrementally and test after each change\n9. [ ] Monitor Hubble metrics for improvements\n\n## Sample Values\n\n```yaml\nkubeProxyReplacement: true\nroutingMode: native\nautoDirectNodeRoutes: true\n\nbpf:\n  datapathMode: netkit\n  masquerade: true\n  tproxy: true\n\nbandwidthManager:\n  enabled: true\n  bbr: true\n\nipv4:\n  enabled: true\nenableIPv4BIGTCP: true\n\nloadBalancer:\n  algorithm: maglev\n  mode: dsr\n  acceleration: native\n\nbgpControlPlane:\n  enabled: true\n\noperator:\n  replicas: 2\n  prometheus:\n    enabled: true\n\nprometheus:\n  enabled: true\n```\n\n## Risks \u0026 Considerations\n\n- **kubeProxyReplacement**: Requires all nodes to have Cilium, no mixed environments\n- **netkit**: Requires Linux kernel 6.8+ (check Talos version)\n- **DSR mode**: May need firewall adjustments for asymmetric routing\n- **BGP**: Requires compatible network infrastructure\n\n## Resources\n\n- [Cilium Performance Tuning](https://docs.cilium.io/en/stable/operations/performance/tuning/)\n- [BPF Datapath Modes](https://docs.cilium.io/en/stable/network/concepts/routing/#datapath-modes)\n- [Bandwidth Manager](https://docs.cilium.io/en/stable/network/kubernetes/bandwidth-manager/)\n- [Maglev Load Balancing](https://docs.cilium.io/en/stable/network/lb-maglev/)","status":"in_progress","priority":2,"issue_type":"feature","owner":"zach.bernstein@smarterdx.com","created_at":"2026-01-17T14:10:47.662429-06:00","created_by":"Zach Bernstein","updated_at":"2026-02-17T14:45:47.110997-06:00"}
